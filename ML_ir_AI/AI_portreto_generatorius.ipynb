{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TK-Problem/Python-mokymai/blob/master/ML_ir_AI/AI_portreto_generatorius.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dreambooth AI sugeneruoto portreto projektas\n",
        "\n",
        "≈†is projektas buvo adaptuotas pagal buildspace [AI Avatar project](https://buildspace.so/builds/ai-avatar) . Rekomenduoju u≈æsiregistruoti ir savaranki≈°kai pereiti per ≈°ƒØ projektƒÖ. Tai padaryti galite nukeliavƒô ƒØ ≈°iƒÖ [nuorodƒÖ](https://buildspace.so/p/build-ai-avatars).\n",
        "\n",
        "Builspace sugeneravo ≈°ƒØ google colab norebook'ƒÖ i≈° kito projekto [Dreambooth diffusers by Shivam Shrirao](https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth).  Apache-2.0 license observed."
      ],
      "metadata": {
        "id": "T0Jm_ptTzpi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. ≈æingsnis - Prisijungti prie virtulios ma≈°inos\n",
        "\n",
        "≈†is projektas skirtas leisti ant google nemokamai prieinam≈≥ grafini≈≥ kort≈≥ (GPU). Pasitikrinkite per `Edit` -> `Notebook settings`, kad yra pasirinkta GPU.\n",
        "\n",
        "\n",
        "![image info](https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-1-4842-4470-8_7/MediaObjects/463852_1_En_7_Fig4_HTML.jpg)\n",
        "\n"
      ],
      "metadata": {
        "id": "vvJfGNtp0YYI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C4NhIiScyC9G"
      },
      "outputs": [],
      "source": [
        "#@markdown ≈†i kodo eilutƒó yra skirta paordyti, kokiƒÖ grafinƒô kortƒÖ ir kiek laisv≈≥ RAM skyrƒó google\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. ≈æingsnis - ƒÆra≈°yti bibliotekas\n",
        "≈Ωemiau esantis kodo blokas yra skirtas ƒØra≈°yti Python bibliotekoms ƒØ virtualiƒÖ aplinkƒÖ."
      ],
      "metadata": {
        "id": "9o9SIPfU5XOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/buildspace/diffusers/main/examples/dreambooth/train_dreambooth.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate==0.15.0 transformers ftfy bitsandbytes==0.35.0 gradio natsort"
      ],
      "metadata": {
        "id": "SrjRheQo6nRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III. ≈æingsnis - Prisijungti prie HuggingFace ü§ó\n",
        "\n",
        "Reikia susikurti paskyrƒÖ HuggingFace, kad galima b≈´t≈≥ i≈° atsisi≈≥sti `Stable Diffusion` paruo≈°tƒÖ modelƒØ. Nueikit ƒØ [HuggingFace](huggingface.co?ref=buildspace) ir susikurkite nemokamƒÖ paskyrƒÖ.\n",
        "\n",
        "Kai susikurtsite paskyrƒÖ, reikia susigeneruoti `read` ≈æetonƒÖ [tokens](https://huggingface.co/settings/tokens) puslapƒØ.\n",
        "\n",
        "![token create](https://hackmd.io/_uploads/BJ1wdTNqi.png)\n",
        "\n",
        "≈†iam projekte naudojame `Stable Diffusion v1.5` sukurtƒÖ `Runway`, jo licenzijƒÖ galite per≈æi≈´rƒóti ƒçia [modelio apra≈°ymas](https://huggingface.co/runwayml/stable-diffusion-v1-5)."
      ],
      "metadata": {
        "id": "R9fqlqdQ-C51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.huggingface\n",
        "HUGGINGFACE_TOKEN = \"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" #@param {type:\"string\"}\n",
        "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uzAOe_B7-IIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV. ≈æingsnis - ƒØsira≈°yti xformers sukompiliuota biblioteka\n",
        "xformers yra papildoma biblioteka, kurios reikia dirbant su kalbos interpretavimu ir teksto clasifikavimu."
      ],
      "metadata": {
        "id": "pAwWY7-yEale"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q https://github.com/brian6091/xformers-wheels/releases/download/0.0.15.dev0%2B4c06c79/xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl\n",
        "# ≈°is kodas buvo sukompiliuotas ant Tesla T4.\n",
        "\n",
        "# jei Jums kodas neveikia, tuomet nuimkite komentarus nuo ≈æemiau esanƒçios eilutƒós\n",
        "# (CTRL + /) ir ƒØra≈°ykite nekompiliuotƒÖ modelƒØ modelƒØ (gali u≈ætrukti kokias 40 min)\n",
        "# %pip install git+https://github.com/facebookresearch/xformers@4c06c79#egg=xformers"
      ],
      "metadata": {
        "id": "Ub3wPFASEl_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V. ≈æingsnis - sukonfiguruoti modelƒØ\n",
        "As mentioned, we're going to be using HuggingFace to load the Stable Diffusion model. There's lots of differently tuned SD models on HF, we're going to stick with the standard v1.5 released by Runway.\n",
        "\n",
        "The way you choose is a model is by putting in the path of the URL on HuggingFace. So `https://huggingface.co/runwayml/stable-diffusion-v1-5` becomes `runwayml/stable-diffusion-v1-5`. \n",
        "\n",
        "You're welcome to try other versions, but we've only tested this on v1.5 and v2.1!"
      ],
      "metadata": {
        "id": "80t_v6WrMLZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Check this if you want to save the weights in your Google Drive  (takes around 4-5 GB).\n",
        "#@markdown We won't need to be doing this. \n",
        "\n",
        "save_to_gdrive = False #@param {type:\"boolean\"}\n",
        "\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Name/path of the initial model.\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the Gdrive directory to save model at.\n",
        "\n",
        "OUTPUT_DIR = \"stable_diffusion_weights/test-version\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4aiWeaqMNG1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VI. ≈æingsnis - sukonfiguruoti resursus modelio pertreniravimui\n",
        "We won't need to touch any of this, but it's here if you want to come back and try turning the knobs once you understand this stuff!\n",
        "\n",
        "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n",
        "\n",
        "\n",
        "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
        "\n",
        "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
        "\n",
        "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
      ],
      "metadata": {
        "id": "PknCbRwSNOFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus - informacija ant kurios pertreniruosi savo modelƒØ\n",
        "Here's where you tell Stable diffusion *what* you're tuning for. \n",
        "\n",
        "**Instance prompt**: this describes exactly what your images are of. In our case it's whatever we decided as the name (\"abraza\" for me) and \"man/woman/person\". This is the **label** for the images we uploaded.\n",
        "\n",
        "**Class prompt**: this just describes what else Stable Diffusion should relate your model to. \"man\", \"woman\" or \"person\" works :)\n",
        "\n",
        "All you need to do is put your unique identifier (\"abraza\") here and whatever the class is right here. Make sure you run both blocks!"
      ],
      "metadata": {
        "id": "Mh5yMe4bNhRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INSTANCE_NAME = 'zdvn' #@param {type:\"string\"}\n",
        "CLASS_NAME = 'woman' #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2oHm3zQ2SSm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
        "\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      f\"photo of {INSTANCE_NAME} {CLASS_NAME}\",\n",
        "        \"class_prompt\":         f\"photo of a {CLASS_NAME}\",\n",
        "        \"instance_data_dir\":    f\"/content/data/{INSTANCE_NAME}\",\n",
        "        \"class_data_dir\":       f\"/content/data/{CLASS_NAME}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# `class_data_dir` contains regularization images\n",
        "import json\n",
        "import os\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ],
      "metadata": {
        "id": "CIqIHWxmNzzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus - paveiksliuk≈≥ ƒØkƒólimas"
      ],
      "metadata": {
        "id": "lUjOghqhN6jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Upload your images by running this cell (recommended). \n",
        "\n",
        "#@markdown Run this block and the \"choose files\" button will pop up. Remember - no more than 10 pictures!\n",
        "\n",
        "#@markdown OR\n",
        "\n",
        "#@markdown You can use the file manager on the left panel to upload (drag and drop) to each `instance_data_dir` (it uploads faster)\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "for c in concepts_list:\n",
        "    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
        "        shutil.move(filename, dst_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sqG8Cbf8N4Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VII. ≈æingsnis -treniruoti modelƒØ!\n",
        "Okay this may seem intimidating, but you don't have to touch most of it!\n",
        "\n",
        "Again, I've left these in here if you really know what you're doing and want to customise your model, for your first time all you need to do is:\n",
        "1. Change `max_train_steps`. You wanna keep this number lower than 2000 - the higher it goes, the longer training takes and the more \"familiar\" SD becomes with you. Keep this number small to avoid overfitting. The general rule of thumb here is 100 steps for each picture+ a base of100. So for 6 pictures, just set it to 700! \n",
        "2. **Update `save_sample_prompt` to a prompt with your subject.** Right after training, this block will generate 4 images of you with this prompt. I recommend spazzing it up a bit more than just \"Photo of xyz person\", those come out quite boring. Put those prompting skills to use!\n",
        "\n",
        "### This will take ~20m to run!"
      ],
      "metadata": {
        "id": "-IvZU4TNQi01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=50 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=500 \\\n",
        "  --save_interval=10000 \\\n",
        "  --save_sample_prompt=\"portrait of zdvn\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
      ],
      "metadata": {
        "id": "NYGHc9IzQsBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7.3 - Generate test images!"
      ],
      "metadata": {
        "id": "jFUUUhbcQ5Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Specify the weights directory to use (leave blank for latest)\n",
        "WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r3PKcbhoTqGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Run to generate a grid of preview images from the last saved weights.\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "weights_folder = OUTPUT_DIR\n",
        "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
        "\n",
        "row = len(folders)\n",
        "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
        "scale = 4\n",
        "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "    folder_path = os.path.join(weights_folder, folder)\n",
        "    image_folder = os.path.join(folder_path, \"samples\")\n",
        "    images = [f for f in os.listdir(image_folder)]\n",
        "    for j, image in enumerate(images):\n",
        "        if row == 1:\n",
        "            currAxes = axes[j]\n",
        "        else:\n",
        "            currAxes = axes[i, j]\n",
        "        if i == 0:\n",
        "            currAxes.set_title(f\"Image {j}\")\n",
        "        if j == 0:\n",
        "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        img = mpimg.imread(image_path)\n",
        "        currAxes.imshow(img, cmap='gray')\n",
        "        currAxes.axis('off')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.savefig('grid.png', dpi=72)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "51USBbZ8Q5qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8 - Convert weights to CKPT\n",
        "Since we want to use our fancy new tuned SD with web apps, we'll have to convert it to CKPT. \n",
        "\n",
        "If you want to save the CKPT file to your GDrive for the future and are running out of space, you can convert it to fp16, which halves the size but also severely degrades the quality. I recommend **leaving it unchecked** cause we're just going to upload it to HuggingFace at the end. "
      ],
      "metadata": {
        "id": "Phqr3bjpRCQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Run this block to start the conversion (necessary)\n",
        "ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
        "#@markdown ----\n",
        "#@markdown  Check this box to convert to fp16, takes half the space (2GB). Not necessary and not recommmended.\n",
        "half_arg = \"\"\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qHQxZ0d-RB7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "j7a9zO-RRUVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "\n",
        "g_cuda = None"
      ],
      "metadata": {
        "id": "dpXDQBmKRUnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Can set random seed here for reproducibility.\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "l97cFMkiRXOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Run Gradio UI for generating images.\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        return pipe(\n",
        "                prompt, height=int(height), width=int(width),\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_images_per_prompt=int(num_samples),\n",
        "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "                generator=g_cuda\n",
        "            ).images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=50)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cY-R0qxORaMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rTfGGOjHXfQP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}